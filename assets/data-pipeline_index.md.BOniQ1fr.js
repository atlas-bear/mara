import{_ as t,c as a,o as n,ag as i}from"./chunks/framework.BONmpqY0.js";const h=JSON.parse('{"title":"Data Pipeline Overview","description":"","frontmatter":{},"headers":[],"relativePath":"data-pipeline/index.md","filePath":"data-pipeline/index.md"}'),s={name:"data-pipeline/index.md"};function o(r,e,l,c,d,p){return n(),a("div",null,e[0]||(e[0]=[i(`<h1 id="data-pipeline-overview" tabindex="-1">Data Pipeline Overview <a class="header-anchor" href="#data-pipeline-overview" aria-label="Permalink to &quot;Data Pipeline Overview&quot;">​</a></h1><p>The MARA data pipeline is a critical component that automates the collection, processing, and enrichment of maritime incident data from multiple authoritative sources. This system transforms diverse, unstructured reports into standardized, structured incident records that power MARA&#39;s analytics and reporting capabilities.</p><h2 id="system-architecture" tabindex="-1">System Architecture <a class="header-anchor" href="#system-architecture" aria-label="Permalink to &quot;System Architecture&quot;">​</a></h2><p>The MARA data pipeline consists of three primary stages, each implemented as Netlify serverless functions:</p><ol><li><strong>Data Collection</strong> - Source-specific functions that gather raw incident data</li><li><strong>Cross-Source Deduplication</strong> - Identifies and merges duplicate reports across sources</li><li><strong>Data Processing</strong> - Transforms raw data into structured incident records</li></ol><p>All stages are orchestrated through scheduled execution in Netlify, forming an automated end-to-end pipeline.</p><h2 id="data-collection-stage" tabindex="-1">Data Collection Stage <a class="header-anchor" href="#data-collection-stage" aria-label="Permalink to &quot;Data Collection Stage&quot;">​</a></h2><p>The collection stage is implemented as a set of source-specific functions that interface with different maritime reporting organizations:</p><table tabindex="0"><thead><tr><th>Function</th><th>Source</th><th>Description</th></tr></thead><tbody><tr><td><code>collect-recaap.js</code></td><td>ReCAAP</td><td>Regional Cooperation Agreement on Combating Piracy and Armed Robbery against Ships in Asia</td></tr><tr><td><code>collect-ukmto.js</code></td><td>UKMTO</td><td>UK Maritime Trade Operations</td></tr><tr><td><code>collect-mdat.js</code></td><td>MDAT-GoG</td><td>Maritime Domain Awareness for Trade - Gulf of Guinea</td></tr><tr><td><code>collect-icc.js</code></td><td>ICC-IMB</td><td>International Chamber of Commerce - International Maritime Bureau</td></tr><tr><td><code>collect-cwd.js</code></td><td>CWD</td><td>Commercial Wisdom Database</td></tr></tbody></table><p>Each collection function:</p><ul><li>Connects to the respective source API or scrapes the source website</li><li>Extracts incident information in the source&#39;s native format</li><li>Standardizes timestamps, coordinates, and key fields</li><li>Stores raw data in the <code>raw_data</code> table in Airtable</li><li>Preserves the original source data structure in the <code>raw_json</code> field</li></ul><h2 id="cross-source-deduplication-stage" tabindex="-1">Cross-Source Deduplication Stage <a class="header-anchor" href="#cross-source-deduplication-stage" aria-label="Permalink to &quot;Cross-Source Deduplication Stage&quot;">​</a></h2><p>The <code>deduplicate-cross-source-background.js</code> function runs after collection to identify and merge duplicate reports:</p><ul><li>Analyzes recently collected raw data across different sources</li><li>Performs multi-dimensional similarity scoring (time, location, vessel, incident type)</li><li>Identifies potential matches and selects primary records</li><li>Merges complementary information while preserving source attribution</li><li>Sets up relationships between merged records</li></ul><p>For detailed information on the deduplication system, see the <a href="/mara/deduplication/">Cross-Source Deduplication documentation</a>.</p><h2 id="data-processing-stage" tabindex="-1">Data Processing Stage <a class="header-anchor" href="#data-processing-stage" aria-label="Permalink to &quot;Data Processing Stage&quot;">​</a></h2><p>The final stage transforms deduplicated raw data into standardized incident records:</p><ol><li><strong>Raw Data Selection</strong> - <code>process-raw-data-background.js</code> selects unprocessed raw data records</li><li><strong>Data Enrichment</strong> - Enhances data with LLM-powered analysis</li><li><strong>Incident Creation</strong> - Creates structured incident records in the <code>incident</code> table</li><li><strong>Vessel Association</strong> - Links incidents to vessel records through the <code>incident_vessel</code> join table</li><li><strong>Reference Data Management</strong> - Maintains reference data for incident types, weapons, etc.</li></ol><h3 id="data-enrichment" tabindex="-1">Data Enrichment <a class="header-anchor" href="#data-enrichment" aria-label="Permalink to &quot;Data Enrichment&quot;">​</a></h3><p>The processing stage incorporates AI-based enrichment using Claude, which:</p><ul><li>Generates concise, descriptive titles for incidents</li><li>Extracts location information when not explicitly provided</li><li>Identifies weapons used, number of attackers, and items stolen</li><li>Creates insightful analysis and security recommendations</li><li>Categorizes incidents based on description content</li></ul><h3 id="reference-data-management" tabindex="-1">Reference Data Management <a class="header-anchor" href="#reference-data-management" aria-label="Permalink to &quot;Reference Data Management&quot;">​</a></h3><p>The system maintains several reference data tables for standardization:</p><ul><li><code>incident_type</code> - Categorization of maritime security events</li><li><code>weapons</code> - Types of weapons used in incidents</li><li><code>items_stolen</code> - Categories of items taken during incidents</li><li><code>response_type</code> - Types of responses to incidents</li><li><code>authorities_notified</code> - Organizations notified about incidents</li></ul><h2 id="data-flow" tabindex="-1">Data Flow <a class="header-anchor" href="#data-flow" aria-label="Permalink to &quot;Data Flow&quot;">​</a></h2><p>The complete data flow through the pipeline is:</p><ol><li><strong>Data Collection</strong>: Source-specific collectors run on schedule (every 30 minutes)</li><li><strong>Raw Data Storage</strong>: Collected data is stored in the <code>raw_data</code> table</li><li><strong>Deduplication</strong>: The deduplication function runs (hourly)</li><li><strong>Processing Trigger</strong>: Deduplication triggers the processing function</li><li><strong>Data Processing</strong>: Raw data is processed into incident records</li><li><strong>Enrichment</strong>: Incident data is enriched with AI-generated analysis</li><li><strong>Standardization</strong>: Data is standardized using reference tables</li><li><strong>Flash Reports</strong>: New incidents trigger flash report generation</li><li><strong>Weekly Reports</strong>: Incidents are aggregated into weekly reports</li></ol><h2 id="scheduling" tabindex="-1">Scheduling <a class="header-anchor" href="#scheduling" aria-label="Permalink to &quot;Scheduling&quot;">​</a></h2><p>The pipeline components are scheduled in Netlify to ensure efficient processing:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[functions.&quot;collect-recaap&quot;]</span></span>
<span class="line"><span>schedule = &quot;0,30 * * * *&quot;  # Every 30 minutes</span></span>
<span class="line"><span></span></span>
<span class="line"><span>[functions.&quot;collect-ukmto&quot;]</span></span>
<span class="line"><span>schedule = &quot;5,35 * * * *&quot;  # 5 and 35 minutes past the hour</span></span>
<span class="line"><span></span></span>
<span class="line"><span>[functions.&quot;collect-mdat&quot;]</span></span>
<span class="line"><span>schedule = &quot;15,45 * * * *&quot;  # 15 and 45 minutes past the hour</span></span>
<span class="line"><span></span></span>
<span class="line"><span>[functions.&quot;collect-icc&quot;]</span></span>
<span class="line"><span>schedule = &quot;20,50 * * * *&quot;  # 20 and 50 minutes past the hour</span></span>
<span class="line"><span></span></span>
<span class="line"><span>[functions.&quot;deduplicate-cross-source-background&quot;]</span></span>
<span class="line"><span>schedule = &quot;28 * * * *&quot;    # 28 minutes past every hour</span></span>
<span class="line"><span>background = true</span></span>
<span class="line"><span></span></span>
<span class="line"><span>[functions.&quot;process-incidents&quot;]</span></span>
<span class="line"><span>schedule = &quot;25,55 * * * *&quot;  # 25 and 55 minutes past the hour</span></span></code></pre></div><p>The staggered scheduling prevents resource contention and ensures the pipeline stages execute in the proper sequence.</p><h2 id="system-benefits" tabindex="-1">System Benefits <a class="header-anchor" href="#system-benefits" aria-label="Permalink to &quot;System Benefits&quot;">​</a></h2><p>This automated data pipeline delivers several key benefits:</p><ol><li><strong>Comprehensive Coverage</strong>: Collects data from all major maritime reporting organizations</li><li><strong>Data Quality</strong>: Eliminates duplicates and standardizes information across sources</li><li><strong>Enriched Analysis</strong>: Adds value through AI-powered incident analysis</li><li><strong>Operational Efficiency</strong>: Minimizes manual data entry and processing</li><li><strong>Timeliness</strong>: Provides near real-time incident updates</li><li><strong>Scalability</strong>: Handles increasing data volumes and additional sources</li><li><strong>Flexibility</strong>: Easily adaptable to accommodate new data formats or sources</li></ol>`,34)]))}const g=t(s,[["render",o]]);export{h as __pageData,g as default};
